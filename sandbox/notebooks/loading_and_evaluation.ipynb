{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6070fca7-c664-4e34-9a76-2cc130749c89",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ANTENNAE solution for Modulation Classification in ITU ML/5G Challenge\n",
    "This notebook contains the ANTENNAE procedure to load and evaluate a trained Quantized Neural Network model that solves the [Lightning-Fast Modulation Classification with Hardware-Efficient Neural Networks](http://bit.ly/brevitas-radioml-challenge-21) problem statement of the [**ITU AI/ML in 5G Challenge**](https://aiforgood.itu.int/ai-ml-in-5g-challenge/). This notebook is an adaptation of the baseline notebook provided by the challenge organizer that can be found at https://github.com/Xilinx/brevitas-radioml-challenge-21. \n",
    "\n",
    "## Outline\n",
    "* [Load the RadioML 2018 Dataset](#load_dataset)\n",
    "* [Define the quantized ANTENNAE QNN Model](#define_model)\n",
    "    * [Load Pre-Trained the ANTENNAE QNN Model](#load_trained_model)\n",
    "* [Evaluate the Accuracy](#evaluate_accuracy)\n",
    "* [Evaluate the Inference Cost](#evaluate_inference_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "written-worker",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some general modules\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2408658c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select which GPU to use (if available)\n",
    "gpu = 0\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.device(gpu)\n",
    "    print(\"Using GPU %d\" % gpu)\n",
    "else:\n",
    "    gpu = None\n",
    "    print(\"Using CPU only\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affiliated-chrome",
   "metadata": {},
   "source": [
    "# The RadioML 2018 Dataset <a id='load_dataset'></a>\n",
    "This problem statement is based on the popular RadioML 2018.01A dataset provided by DeepSig. It is the latest in a series of modulation classification datasets ([deepsig.ai/datasets](https://www.deepsig.ai/datasets)) and contains samples of 24 digital and analog modulation types under various channel impairments and signal-to-noise ratios (SNRs). For more information on the dataset origins, we refer to the associated paper [Over-the-Air Deep Learning Based Radio Signal Classification](https://arxiv.org/pdf/1712.04578.pdf) by O’Shea, Roy, and Clancy.\n",
    "\n",
    "\n",
    "The dataset comes in hdf5 format and exhibits the following structure:\n",
    "- 24 modulations\n",
    "- 26 SNRs per modulation (-20 dB through +30 dB in steps of 2)\n",
    "- 4096 frames per modulation-SNR combination\n",
    "- 1024 complex time-series samples per frame\n",
    "- Samples as floating point in-phase and quadrature (I/Q) components, resulting in a (1024,2) frame shape\n",
    "- 2.555.904 frames in total\n",
    " \n",
    "\n",
    "## Download\n",
    "The dataset is available here: **https://opendata.deepsig.io/datasets/2018.01/2018.01.OSC.0001_1024x2M.h5.tar.gz**\n",
    "\n",
    "Since access requires a (straightforward) registration, you must download and extract it manually. It measures about 18 GiB in size (20 GiB uncompressed).\n",
    "\n",
    "To access it from within this container, you can place it:\n",
    "- A) Under the sandbox directory you launched this notebook from, which is mounted under \"/workspace/sandbox\".\n",
    "- B) Anywhere, then set the environment variable `DATASET_DIR` before launching \"run_docker.sh\" to mount it under \"/workspace/dataset\".\n",
    "\n",
    "You might notice that the dataset comes with a \"classes.txt\" file containing the alleged modulation labels. However, you should disregard the ordering of these labels due to a known issue ([github.com/radioML/dataset/issues/25](http://github.com/radioML/dataset/issues/25)). This notebook uses the corrected labels throughout.\n",
    "\n",
    "In the following, we create the data loader and can inspect some frames to get an idea what the input data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f18d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if dataset is present\n",
    "import os.path\n",
    "dataset_path = \"./dataset/2018.01/GOLD_XYZ_OSC.0001_1024.hdf5\"\n",
    "os.path.isfile(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "happy-place",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data loader\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import h5py\n",
    "\n",
    "class radioml_18_dataset(Dataset):\n",
    "    def __init__(self, dataset_path):\n",
    "        super(radioml_18_dataset, self).__init__()\n",
    "        h5_file = h5py.File(dataset_path,'r')\n",
    "        self.data = h5_file['X']\n",
    "        self.mod = np.argmax(h5_file['Y'], axis=1) # comes in one-hot encoding\n",
    "        self.snr = h5_file['Z'][:,0]\n",
    "        self.len = self.data.shape[0]\n",
    "\n",
    "        self.mod_classes = ['OOK','4ASK','8ASK','BPSK','QPSK','8PSK','16PSK','32PSK',\n",
    "        '16APSK','32APSK','64APSK','128APSK','16QAM','32QAM','64QAM','128QAM','256QAM',\n",
    "        'AM-SSB-WC','AM-SSB-SC','AM-DSB-WC','AM-DSB-SC','FM','GMSK','OQPSK']\n",
    "        self.snr_classes = np.arange(-20., 32., 2) # -20dB to 30dB\n",
    "\n",
    "        # do not touch this seed to ensure the prescribed train/test split!\n",
    "        np.random.seed(2018)\n",
    "        train_indices = []\n",
    "        test_indices = []\n",
    "        for mod in range(0, 24): # all modulations (0 to 23)\n",
    "            for snr_idx in range(0, 26): # all SNRs (0 to 25 = -20dB to +30dB)\n",
    "                # 'X' holds frames strictly ordered by modulation and SNR\n",
    "                start_idx = 26*4096*mod + 4096*snr_idx\n",
    "                indices_subclass = list(range(start_idx, start_idx+4096))\n",
    "                \n",
    "                # 90%/10% training/test split, applied evenly for each mod-SNR pair\n",
    "                split = int(np.ceil(0.1 * 4096)) \n",
    "                np.random.shuffle(indices_subclass)\n",
    "                train_indices_subclass = indices_subclass[split:]\n",
    "                test_indices_subclass = indices_subclass[:split]\n",
    "                              \n",
    "                # you could train on a subset of the data, e.g. based on the SNR\n",
    "                # here we use all available training samples\n",
    "                if snr_idx >= 0:\n",
    "                    train_indices.extend(train_indices_subclass)\n",
    "                test_indices.extend(test_indices_subclass)\n",
    "                \n",
    "        self.train_sampler = torch.utils.data.SubsetRandomSampler(train_indices)\n",
    "        self.test_sampler = torch.utils.data.SubsetRandomSampler(test_indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # transpose frame into Pytorch channels-first format (NCL = -1,2,1024)\n",
    "        return self.data[idx].transpose(), self.mod[idx], self.snr[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "dataset = radioml_18_dataset(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0602f1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect a frame\n",
    "mod = 12 # 0 to 23\n",
    "snr_idx = 25 # 0 to 25 = -20dB to +30dB\n",
    "sample = 123 # 0 to 4095\n",
    "#-----------------------#\n",
    "idx = 26*4096*mod + 4096*snr_idx + sample\n",
    "data, mod, snr = dataset.data[idx], dataset.mod[idx], dataset.snr[idx]\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(data)\n",
    "print(\"Modulation: %s, SNR: %.1f dB, Index: %d\" % (dataset.mod_classes[mod], snr, idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccccb1d",
   "metadata": {},
   "source": [
    "# The ANTENNAE QNN Model <a id='define_model'></a>\n",
    "\n",
    "The proposed solution apply 4-bit quantization to the activations and a combination of 4-bit and 8-bit quantization to weights of the NN layers, except for the final classification output. The input data is quantized to 8 bits with a dedicated quantization layer. SImilar to the baseline model provided by Xilins, we set a fixed quantization range (-2.0, 2.0).\n",
    "\n",
    "For more information on Brevitas you can turn to these resources:\n",
    "- [GitHub repository](https://github.com/Xilinx/brevitas)\n",
    "- [Tutorial notebooks](https://github.com/Xilinx/brevitas/tree/master/notebooks)\n",
    "- [Example models](https://github.com/Xilinx/brevitas/tree/master/src/brevitas_examples)\n",
    "- Public discussion in the [Brevitas Gitter channel](https://gitter.im/xilinx-brevitas/community)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b7f38e-3ac2-4a4f-8795-10751eb8dbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import brevitas.nn as qnn\n",
    "from brevitas.quant import IntBias\n",
    "from brevitas.inject.enum import ScalingImplType\n",
    "from brevitas.inject.defaults import Int8ActPerTensorFloatMinMaxInit\n",
    "\n",
    "# Adjustable hyperparameters\n",
    "input_bits = 8\n",
    "a_bits = 4\n",
    "w_bits = 8\n",
    "w2_bits = 4\n",
    "\n",
    "# Setting seeds for reproducibility\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "class InputQuantizer(Int8ActPerTensorFloatMinMaxInit):\n",
    "    bit_width = input_bits\n",
    "    min_val = -2.0\n",
    "    max_val = 2.0\n",
    "    scaling_impl_type = ScalingImplType.CONST # Fix the quantization range to [min_val, max_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "motivated-freight",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class model(nn.Sequential):\n",
    "    def __init__(self):\n",
    "        super(model, self).__init__()\n",
    "        self.conv1 = qnn.QuantConv1d(2, 64, 3, padding=1, weight_bit_width=w_bits, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.relu1 = qnn.QuantReLU(bit_width=a_bits)\n",
    "        self.maxpol1 = nn.MaxPool1d(2)\n",
    "\n",
    "        self.conv2 = qnn.QuantConv1d(64, 32, 3, padding=1, weight_bit_width=w_bits, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(32)\n",
    "        self.relu2 = qnn.QuantReLU(bit_width=a_bits)\n",
    "        self.maxpol2 = nn.MaxPool1d(2)\n",
    "\n",
    "        self.conv3 = qnn.QuantConv1d(32, 32, 3, padding=1, weight_bit_width=w_bits, bias=False)\n",
    "        self.bn3 = nn.BatchNorm1d(32)\n",
    "        self.relu3 = qnn.QuantReLU(bit_width=a_bits)\n",
    "        self.maxpol3 = nn.MaxPool1d(2)\n",
    "        \n",
    "        self.conv4 = qnn.QuantConv1d(32, 32, 3, padding=1, weight_bit_width=w_bits, bias=False)\n",
    "        self.bn4 = nn.BatchNorm1d(32)\n",
    "        self.relu4 = qnn.QuantReLU(bit_width=a_bits)\n",
    "        self.maxpol4 = nn.MaxPool1d(2)\n",
    "        \n",
    "        self.conv5 = qnn.QuantConv1d(32, 32, 3, padding=1, weight_bit_width=w2_bits, bias=False)\n",
    "        self.bn5 = nn.BatchNorm1d(32)\n",
    "        self.relu5 = qnn.QuantReLU(bit_width=a_bits)\n",
    "        self.maxpol5 = nn.MaxPool1d(4)    \n",
    "        \n",
    "        self.Flatten = nn.Flatten()\n",
    "        \n",
    "        self.linear1 =  qnn.QuantLinear(256,32, weight_bit_width=w2_bits, bias=False)\n",
    "        self.BatchNorm1d1 =  nn.BatchNorm1d(32)\n",
    "        self.reluLinear1 = qnn.QuantReLU(bit_width=a_bits, return_quant_tensor=True)\n",
    "        \n",
    "        self.linear3  = qnn.QuantLinear(32, 24, weight_bit_width=w_bits, bias=True, bias_quant=IntBias)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        qnn.QuantHardTanh(act_quant=InputQuantizer)\n",
    "        x = F.avg_pool1d(input, kernel_size=2, stride=2)\n",
    "        output = self.maxpol1(self.relu1(self.bn1(self.conv1(x))))  \n",
    "        output = self.maxpol2(self.relu2(self.bn2(self.conv2(output))))\n",
    "        output = self.maxpol3(self.relu3(self.bn3(self.conv3(output))))\n",
    "        output = self.maxpol4(self.relu4(self.bn4(self.conv4(output))))\n",
    "        output = self.maxpol5(self.relu5(self.bn5(self.conv5(output))))\n",
    "        output = self.Flatten(output)\n",
    "\n",
    "        output = self.reluLinear1(self.BatchNorm1d1(self.linear1(output)))\n",
    "        output = self.linear3(output)      \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c6a3ab",
   "metadata": {},
   "source": [
    "# Load a Trained Model <a id='load_trained_model'></a>\n",
    "Let's load the pre-trained model.\n",
    "It was trained for 100 epochs and reaches an overall accuracy of 56.6%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5c438f-d478-4991-91cf-d3f2f6d6a08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained parameters\n",
    "models_path = './models/'\n",
    "prefix_output_filename = 'ANTENNAE_QNN_model_submitted'\n",
    "savefile = models_path+prefix_output_filename+'.pth'\n",
    "\n",
    "# Check if model is present\n",
    "os.path.isfile(savefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c1d2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model()\n",
    "saved_state = torch.load(savefile, map_location=torch.device(\"cpu\")) \n",
    "model.load_state_dict(saved_state)\n",
    "\n",
    "if gpu is not None:\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "small-blame",
   "metadata": {},
   "source": [
    "# Evaluate the Accuracy <a id='evaluate_accuracy'></a>\n",
    "The following cells visualize the test accuracy across different modulations and signal-to-noise ratios. Submissions for this problem statement must reach an overall accuracy of at least **56.0%**, so this should give you an idea what makes up this figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aed62de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a fresh test data loader\n",
    "batch_size = 2048 #We use 2048 for training in Tesla V100 and 512 for GTX 1060.\n",
    "dataset = radioml_18_dataset(dataset_path)\n",
    "data_loader_test = DataLoader(dataset, batch_size=batch_size, sampler=dataset.test_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1267b431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference on validation data\n",
    "y_exp = np.empty((0))\n",
    "y_snr = np.empty((0))\n",
    "y_pred = np.empty((0,len(dataset.mod_classes)))\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(data_loader_test, desc=\"Batches\"):\n",
    "        inputs, target, snr = data\n",
    "        if gpu is not None:\n",
    "            inputs = inputs.cuda()\n",
    "        output = model(inputs)\n",
    "        y_pred = np.concatenate((y_pred,output.cpu()))\n",
    "        y_exp = np.concatenate((y_exp,target))\n",
    "        y_snr = np.concatenate((y_snr,snr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d5abcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot overall confusion matrix\n",
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues, labels=[]):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(labels))\n",
    "    plt.xticks(tick_marks, labels, rotation=90)\n",
    "    plt.yticks(tick_marks, labels)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "conf = np.zeros([len(dataset.mod_classes),len(dataset.mod_classes)])\n",
    "confnorm = np.zeros([len(dataset.mod_classes),len(dataset.mod_classes)])\n",
    "for i in range(len(y_exp)):\n",
    "    j = int(y_exp[i])\n",
    "    k = int(np.argmax(y_pred[i,:]))\n",
    "    conf[j,k] = conf[j,k] + 1\n",
    "for i in range(0,len(dataset.mod_classes)):\n",
    "    confnorm[i,:] = conf[i,:] / np.sum(conf[i,:])\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plot_confusion_matrix(confnorm, labels=dataset.mod_classes)\n",
    "\n",
    "cor = np.sum(np.diag(conf))\n",
    "ncor = np.sum(conf) - cor\n",
    "print(\"Overall Accuracy across all SNRs: %f\"%(cor / (cor+ncor)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b63a479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrices at 4 different SNRs\n",
    "snr_to_plot = [-20,-4,+4,+30]\n",
    "plt.figure(figsize=(16,10))\n",
    "acc = []\n",
    "for snr in dataset.snr_classes:\n",
    "    # extract classes @ SNR\n",
    "    indices_snr = (y_snr == snr).nonzero()\n",
    "    y_exp_i = y_exp[indices_snr]\n",
    "    y_pred_i = y_pred[indices_snr]\n",
    " \n",
    "    conf = np.zeros([len(dataset.mod_classes),len(dataset.mod_classes)])\n",
    "    confnorm = np.zeros([len(dataset.mod_classes),len(dataset.mod_classes)])\n",
    "    for i in range(len(y_exp_i)):\n",
    "        j = int(y_exp_i[i])\n",
    "        k = int(np.argmax(y_pred_i[i,:]))\n",
    "        conf[j,k] = conf[j,k] + 1\n",
    "    for i in range(0,len(dataset.mod_classes)):\n",
    "        confnorm[i,:] = conf[i,:] / np.sum(conf[i,:])\n",
    " \n",
    "    if snr in snr_to_plot:\n",
    "        plot, = np.where(snr_to_plot == snr)[0]\n",
    "        plt.subplot(221+plot)\n",
    "        plot_confusion_matrix(confnorm, labels=dataset.mod_classes, title=\"Confusion Matrix @ %d dB\"%(snr))\n",
    " \n",
    "    cor = np.sum(np.diag(conf))\n",
    "    ncor = np.sum(conf) - cor\n",
    "    acc.append(cor/(cor+ncor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712b1154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy over SNR\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(dataset.snr_classes, acc, marker='o')\n",
    "plt.xlabel(\"SNR [dB]\")\n",
    "plt.xlim([-20, 30])\n",
    "plt.ylabel(\"Classification Accuracy\")\n",
    "plt.yticks(np.arange(0, 1.1, 0.1))\n",
    "plt.title(\"Classification Accuracy over SNR\")\n",
    "plt.grid()\n",
    "plt.title(\"Classification Accuracy over SNR\");\n",
    "\n",
    "print(\"Accuracy @ highest SNR (+30 dB): %f\"%(acc[-1]))\n",
    "print(\"Accuracy overall: %f\"%(np.mean(acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eda0e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy per modulation\n",
    "accs = []\n",
    "for mod in range(24):\n",
    "    accs.append([])\n",
    "    for snr in dataset.snr_classes:\n",
    "        indices = ((y_exp == mod) & (y_snr == snr)).nonzero()\n",
    "        y_exp_i = y_exp[indices]\n",
    "        y_pred_i = y_pred[indices]\n",
    "        cor = np.count_nonzero(y_exp_i == np.argmax(y_pred_i, axis=1))\n",
    "        accs[mod].append(cor/len(y_exp_i))\n",
    "        \n",
    "# Plot accuracy-over-SNR curve\n",
    "plt.figure(figsize=(12,8))\n",
    "for mod in range(24):\n",
    "    if accs[mod][25] < 0.95 or accs[mod][0] > 0.1:\n",
    "        color = None\n",
    "    else:\n",
    "        color = \"black\"\n",
    "    plt.plot(dataset.snr_classes, accs[mod], label=str(mod) + \": \" + dataset.mod_classes[mod], color=color)\n",
    "plt.xlabel(\"SNR [dB]\")\n",
    "plt.xlim([-20, 30])\n",
    "plt.ylabel(\"Classification Accuracy\")\n",
    "plt.yticks(np.arange(0, 1.1, 0.1))\n",
    "plt.title(\"Accuracy breakdown\")\n",
    "plt.grid()\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeefa8b7",
   "metadata": {},
   "source": [
    "# Evaluate the Inference Cost <a id='evaluate_inference_cost'></a>\n",
    "\n",
    "First, we have to export the model to Brevita's quantized variant of the ONNX interchange format. **All submissions must correctly pass through this export flow and provide the resulting .onnx file**. Any `TracerWarning` can be safely ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef9cb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from brevitas.export.onnx.generic.manager import BrevitasONNXManager\n",
    "\n",
    "export_onnx_path = \"./models/\"+prefix_output_filename+\"_model_export.onnx\"\n",
    "final_onnx_path = \"./models/\"+prefix_output_filename+\"_model_final.onnx\"\n",
    "cost_dict_path = \"./models/\"+prefix_output_filename+\"_model_cost.json\"\n",
    "\n",
    "BrevitasONNXManager.export(model.cpu(), input_t=torch.randn(1, 2, 1024), export_path=export_onnx_path);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c6e29c",
   "metadata": {},
   "source": [
    "Now we use our analysis tool, which is part of [finn-base](https://github.com/Xilinx/finn-base), to determine the inference cost. It reports the number of output activation variables (`mem_o`), weight parameters (`mem_w`), and multiply-accumulate operations (`op_mac`) for each data type. These are used to calculate the total number of activation bits, weight bits, and bit-operations (BOPS).\n",
    "\n",
    "If the report shows any unsupported operations, for instance because you implemented custom layers, you should check with the rules on the problem statement [website](http://bit.ly/brevitas-radioml-challenge-21) and consider to contact the organizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40dc8bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn.util.inference_cost import inference_cost\n",
    "import json\n",
    "\n",
    "inference_cost(export_onnx_path, output_json=cost_dict_path, output_onnx=final_onnx_path,\n",
    "               preprocess=True, discount_sparsity=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9fcc4e",
   "metadata": {},
   "source": [
    "The call to `ìnference_cost()` cleans up the model by inferring shapes and datatypes, folding constants, etc. We visualize the pre-processed ONNX model using [Netron](https://netron.app/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1777a6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import netron\n",
    "from IPython.display import IFrame\n",
    "\n",
    "def showInNetron(model_filename):\n",
    "    localhost_url = os.getenv(\"LOCALHOST_URL\")\n",
    "    netron_port = os.getenv(\"NETRON_PORT\")\n",
    "    netron.start(model_filename, address=(\"0.0.0.0\", int(netron_port)))\n",
    "    return IFrame(src=\"http://%s:%s/\" % (localhost_url, netron_port), width=\"100%\", height=400)\n",
    "\n",
    "showInNetron(final_onnx_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d3ecfb",
   "metadata": {},
   "source": [
    "Finally, we compute the inference cost score, normalized to the baseline 8-bit VGG10 defined in this notebook. **Submissions will be judged based on this score.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a707479",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(cost_dict_path, 'r') as f:\n",
    "    inference_cost_dict = json.load(f)\n",
    "\n",
    "bops = int(inference_cost_dict[\"total_bops\"])\n",
    "w_bits = int(inference_cost_dict[\"total_mem_w_bits\"])\n",
    "\n",
    "bops_baseline = 807699904\n",
    "w_bits_baseline = 1244936\n",
    "\n",
    "score = 0.5*(bops/bops_baseline) + 0.5*(w_bits/w_bits_baseline)\n",
    "print(\"Normalized inference cost score: %f\" % score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
